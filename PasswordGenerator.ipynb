{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passwort Generator\n",
    "Das Projekt hat sich die Erstellung eines Passwort-Generators als Ziel gesetzt. Dabei sollen die zu erstellenden Passwörter möglichst denen eines Menschen ähneln. \n",
    "Zur Realisierung wurde die RNN- Architektur verwendet. Diese Recurrent Neural Networks ermöglichen es, Voraussagen mittels eines Kontext zu treffen, der durch frühere Inputs entstanden ist. Das Netzwerk verfügt sozusagen über ein Gedächtnis. In der Umsetzung geschieht dies durch die Kombination des Hidden-Layern aus der vorherigen Sequenz mit den Hidden-Layern aus der aktuellen Sequenz. Die vorherigen Hidden-Layer haben damit Einfluß auf den Output der nächsten Sequenz. Dieser Algorithmus wird in einer Schleife abgebildet, bis sämtliche Inputs verarbeitet wurden und der Kontext ersichtlich ist.\n",
    "Klarer wird dies mit den nachfolgenden Formeln, mit denen das Netzwerk trainiert wird:\n",
    "\n",
    "\\begin{align}\n",
    "\\ h_t  = f(W^{hh}h_t-1 + W^{hx} + x_t \\\\\n",
    "\\ y_t  = softmax(W^Sh_t) \\\\\n",
    "\\ J^t(\\theta)  =\\sum_{i=1}^{[V]} (y_{ti}\\log(y_{ti}))\n",
    "\\end{align}\n",
    "\n",
    "Die erste Formel ist dafür da, sich an die Hidden-Layer des vorherigen Durchlaufs zu \"erinnern\". Dabei wird durch h-1 auf den vorherigen Hidden-Layer zugegriffen. Dies wird kombiniert mit dem aktuellen x, auch wird anschließend eine Akivierungsfunktion durchgeführt, am gebräuchlisten sind hierbei der Tangens hyperbolicus oder die Sigmoid-Funktion.\n",
    "Die zweite Formel kümmert sich um die Voraussage des nächsten Ergebnisses in Form von einer Wahrscheinlichkeitsverteilung. \n",
    "Zum Schluss wird in der dritten Formel mittels der Cross-Entropy-Loss-Funktion der Fehler zwischen dem Input und dem Output berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import wget # to download passwordlist\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auslesen der Passworddatei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in letters\n",
    "    )\n",
    "\n",
    "def readPasswords(filename):\n",
    "    passwords = []\n",
    "    with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            if len(line) > 1:\n",
    "                passwords.append(line)\n",
    "    passw = [unicodeToAscii(password) for password in passwords]\n",
    "    return passw\n",
    "\n",
    "def charToIndex(char):\n",
    "    return letters.find(char)\n",
    "\n",
    "def passwordToTensor(name):\n",
    "    ret = torch.zeros(len(name), 1, len_letters)\n",
    "    for i, char in enumerate(name):\n",
    "        ret[i][0][charToIndex(char)] = 1\n",
    "    return ret\n",
    "\n",
    "def targetToTensor(password):\n",
    "    indizes = [letters.find(password[i]) for i in range(1,len(password))]\n",
    "    indizes.append(len_letters - 1) #EOS\n",
    "    return torch.LongTensor(indizes)\n",
    "                                                        \n",
    "    \n",
    "letters = string.ascii_letters + string.digits + string.punctuation\n",
    "len_letters = len(letters) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Passwortlisten\n",
    "Download der Listen falls diese nicht existieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful read rockyou.txt\n",
      "total passswords in list:  14344384\n",
      "successful read passwords-20MB.txt\n",
      "total passswords in list:  28688768\n",
      "successful read cracklib.txt\n",
      "total passswords in list:  43033152\n"
     ]
    }
   ],
   "source": [
    "urls = ['https://www.scrapmaker.com/data/wordlists/dictionaries/rockyou.txt', \n",
    "        'https://www.scrapmaker.com/download/data/wordlists/passwords/passwords-20MB.txt',\n",
    "        'https://www.scrapmaker.com/download/data/wordlists/passwords/cracklib.txt',]\n",
    "filelist = []\n",
    "passwords = []\n",
    "\n",
    "# exist files\n",
    "dirs = os.listdir()\n",
    "for file in dirs:\n",
    "    if file.endswith(\".txt\"):\n",
    "        filelist.append(file)\n",
    "\n",
    "for url in urls:\n",
    "    file = url.split(\"/\")[-1]\n",
    "    #download files if not exists\n",
    "    if file not in filelist:\n",
    "        wget.download(url)\n",
    "        print('\\n successful downloaded ', url)\n",
    "    #read file and append to passwordlist\n",
    "    passwords += readPasswords(filename)\n",
    "    print('successful read', file)\n",
    "    print('total passswords in list: ', len(passwords))\n",
    "\n",
    "#entfernen von leeren Zeilen\n",
    "passwords = [passw for passw in passwords if passw != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen\n",
    "    nn.LogSoftmax() - \n",
    "    nn.LeakyReLU() - \n",
    "    nn.LogSigmoid() -\n",
    "    nn.Tanh() - \n",
    "## Generatorklasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "class PasswordGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PasswordGenerator, self).__init__()\n",
    "        self.hidden = hidden_size\n",
    "        self.input2hidden = nn.Linear(input_size + hidden_size, hidden_size, bias=True)\n",
    "        self.input2output = nn.Linear(input_size + hidden_size, output_size, bias=True)\n",
    "        self.output2output = nn.Linear(hidden_size +  output_size, output_size, bias=True)        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined_input = torch.cat((input, hidden), dim=1)\n",
    "        hidden = self.input2hidden(combined_input)\n",
    "        output = self.input2output(combined_input)\n",
    "        combined_output = torch.cat((hidden, output), dim=1)\n",
    "        output = self. output2output(combined_output)        \n",
    "        output = self.dropout(output)\n",
    "        #output = self.softmax(output)\n",
    "        \n",
    "        #output = self.sigmoid(output)\n",
    "        output = self.relu(output)\n",
    "        #output = self.tanh(output)\n",
    "        \n",
    "        return hidden, output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktionen\n",
    "    nn.BCELoss() - Binary Cross Entropy\n",
    "    nn.BCEWithLogitsLoss() - This loss combines a Sigmoid layer and the BCELoss in one single class\n",
    "    nn.NLLLoss() - negative log likelihood loss\n",
    "    nn.CrossEntropyLoss() - combines nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
    "    \n",
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PasswordGenerator(len_letters, len_letters, len_letters)\n",
    "loss_fn = nn.CrossEntropyLoss() #define Loss Function\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def trainPasswords(input, target):\n",
    "    target.unsqueeze_(-1)\n",
    "    hidden = model.initHidden()\n",
    "    model.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    loss = 0\n",
    "    for i in range(input.size()[0]):\n",
    "        hidden, output = model(input[i], hidden)\n",
    "        l = loss_fn(output, target[i]) # Compute the loss\n",
    "        loss += l\n",
    "    loss.requires_grad_(True) # The autograd package provides automatic differentiation for all operations on Tensors\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.add(-learning_rate, p.grad.data)\n",
    "        \n",
    "    return output, loss.item() / input.size(0)\n",
    "\n",
    "def train(trainrounds):\n",
    "    total_loss = 0\n",
    "    plots = []\n",
    "    plot_every = 100\n",
    "    progress = 0\n",
    "    c = 0\n",
    "    \n",
    "    for j in range(0, trainrounds):\n",
    "    #for j in range(len(passwords)):\n",
    "            #password = passwords[j]\n",
    "            password = random.choice(passwords)\n",
    "            #print('picked password:', password)\n",
    "            input = passwordToTensor(password)\n",
    "            target = targetToTensor(password)\n",
    "            output, loss = trainPasswords(input, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            progress = j / trainrounds * 100\n",
    "            if (c < round(progress) and round(progress) % 5 == 0) or j == 1:\n",
    "                c = round(progress)\n",
    "                print(round(progress), '% made. Loss: ', loss)\n",
    "            if j % plot_every == 0:\n",
    "                plots.append(total_loss / plot_every)\n",
    "                total_loss = 0\n",
    "                torch.save(model, filename)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Password:  <zK\"ZoK\";\"Z\n",
      "0 % made. Loss:  4.565231959025065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-5caaf83ed0f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#train(range(len(passwords)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#after train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-0bb5d7f6fd43>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trainrounds)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpasswordToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargetToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainPasswords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-0bb5d7f6fd43>\u001b[0m in \u001b[0;36mtrainPasswords\u001b[1;34m(input, target)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# The autograd package provides automatic differentiation for all operations on Tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_chars = 10 # max 10 chars for password\n",
    "\n",
    "def sample(start_letter='a'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = passwordToTensor(start_letter)\n",
    "        hidden = model.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_chars):\n",
    "            output, hidden = model(input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == len_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = letters[topi]\n",
    "                output_name += letter\n",
    "            input = passwordToTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "#before train\n",
    "random_start_char = random.choice(letters)\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "\n",
    "#train\n",
    "#train(range(len(passwords)))\n",
    "train(100000)\n",
    "\n",
    "#after train\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "print('Sampled Password: ', sample(random_start_char))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
