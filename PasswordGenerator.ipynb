{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passwort Generator\n",
    "Das Projekt hat sich die Erstellung eines Passwort-Generators als Ziel gesetzt. Dabei sollen die zu erstellenden Passwörter möglichst denen eines Menschen ähneln. \n",
    "Zur Realisierung wurde die RNN- Architektur verwendet. Diese Recurrent Neural Networks ermöglichen es, Voraussagen mittels eines Kontext zu treffen, der durch frühere Inputs entstanden ist. Das Netzwerk verfügt sozusagen über ein Gedächtnis. In der Umsetzung geschieht dies durch die Kombination des Hidden-Layern aus der vorherigen Sequenz mit den Hidden-Layern aus der aktuellen Sequenz. Die vorherigen Hidden-Layer haben damit Einfluß auf den Output der nächsten Sequenz. Dieser Algorithmus wird in einer Schleife abgebildet, bis sämtliche Inputs verarbeitet wurden und der Kontext ersichtlich ist.\n",
    "Klarer wird dies mit den nachfolgenden Formeln, mit denen das Netzwerk trainiert wird:\n",
    "\n",
    "\\begin{align}\n",
    "\\ h_t  = f(W^{hh}h_t-1 + W^{hx} + x_t \\\\\n",
    "\\ y_t  = softmax(W^Sh_t) \\\\\n",
    "\\ J^t(\\theta)  =\\sum_{i=1}^{[V]} (y_{ti}\\log(y_{ti}))\n",
    "\\end{align}\n",
    "\n",
    "Die erste Formel ist dafür da, sich an die Hidden-Layer des vorherigen Durchlaufs zu \"erinnern\". Dabei wird durch h-1 auf den vorherigen Hidden-Layer zugegriffen. Dies wird kombiniert mit dem aktuellen x, auch wird anschließend eine Akivierungsfunktion durchgeführt, am gebräuchlisten sind hierbei der Tangens hyperbolicus oder die Sigmoid-Funktion.\n",
    "Die zweite Formel kümmert sich um die Voraussage des nächsten Ergebnisses in Form von einer Wahrscheinlichkeitsverteilung. \n",
    "Zum Schluss wird in der dritten Formel mittels der Cross-Entropy-Loss-Funktion der Fehler zwischen dem Input und dem Output berechnet.\n",
    "\n",
    "/EOS noch erklären, / Quellen angeben / Quellcode mehr kommentieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import wget # to download passwordlist\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auslesen der Passworddatei:\n",
    "\n",
    "Beim Einlesen der Daten haben wir uns an das Tutorial von Pytorch im Kontext mit RNNs gehalten. Um die Strings verarbeiten zu können, haben wir die Methode aus dem Tutorial übernommen, die die Zeichen in Ascii-Code umwandelt, um eine numerische Darstellung der Passwörter zu erhalten. Weiterhin erfolgt eine Umwandlung zu Tensoren sowohl für die Passwörter als auch für die Targets, damit die Backpropagation angewendet werden kann, um den Loss zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in letters\n",
    "    )\n",
    "\n",
    "def readPasswords(filename):\n",
    "    passwords = []\n",
    "    with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            if len(line) > 1:\n",
    "                passwords.append(line)\n",
    "    passw = [unicodeToAscii(password) for password in passwords]\n",
    "    return passw\n",
    "\n",
    "def charToIndex(char):\n",
    "    return letters.find(char)\n",
    "\n",
    "def passwordToTensor(name):\n",
    "    ret = torch.zeros(len(name), 1, len_letters)\n",
    "    for i, char in enumerate(name):\n",
    "        ret[i][0][charToIndex(char)] = 1\n",
    "    return ret\n",
    "\n",
    "def targetToTensor(password):\n",
    "    indizes = [letters.find(password[i]) for i in range(1,len(password))]\n",
    "    indizes.append(len_letters - 1) #EOS\n",
    "    return torch.LongTensor(indizes)\n",
    "                                                        \n",
    "    \n",
    "letters = string.ascii_letters + string.digits + string.punctuation\n",
    "len_letters = len(letters) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Passwortlisten\n",
    "\n",
    "Um unser Projekt möglichst leicht reproduzierbar zu machen, haben wir eine Funktion eingebaut, die automatisch Passwortlisten runterlädt, falls diese noch nicht vorhanden sind. So ist gewährleistet, dass Interessenten, die selbst das Netzwerk trainieren möchten, nicht erst umständlich Passwortlisten runterladen müssen. Die Passwortlisten wurden vorher durch ein Skript von Passwörtern gereinigt, die Zeichen enthielten, die nicht UTF-8 -kompatibel waren, da diese Passwörter beim Einlesen Fehler erzeugt haben. Zusammen kommen wir auf ungefähr 83 Millionen Passwörter, die wir das Netzwerk verwenden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful read rockyou.txt\n",
      "total passswords in list:  1472\n",
      "successful read passwords-20MB.txt\n",
      "total passswords in list:  1527\n",
      "successful read cracklib.txt\n",
      "total passswords in list:  1582\n"
     ]
    }
   ],
   "source": [
    "urls = ['https://www.scrapmaker.com/data/wordlists/dictionaries/rockyou.txt',\n",
    "        'https://www.scrapmaker.com/download/data/wordlists/passwords/thelist.txt']\n",
    "filelist = []\n",
    "passwords = []\n",
    "\n",
    "# exist files\n",
    "dirs = os.listdir()\n",
    "for file in dirs:\n",
    "    if file.endswith(\".txt\"):\n",
    "        filelist.append(file)\n",
    "\n",
    "for url in urls:\n",
    "    file = url.split(\"/\")[-1]\n",
    "    #download files if not exists\n",
    "    if file not in filelist:\n",
    "        wget.download(url)\n",
    "        print('\\n successful downloaded ', url)\n",
    "    #read file and append to passwordlist\n",
    "    passwords += readPasswords(file)\n",
    "    print('successful read', file)\n",
    "    print('total passswords in list: ', len(passwords))\n",
    "\n",
    "#entfernen von leeren Zeilen\n",
    "passwords = [passw for passw in passwords if passw != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen\n",
    "    nn.LogSoftmax() - \n",
    "    nn.LeakyReLU() - \n",
    "    nn.LogSigmoid() -\n",
    "    nn.Tanh() - \n",
    "## Generatorklasse:\n",
    "\n",
    "In der Generatorklasse werden die grundlegenden Funktionen und Variablen festgelegt, mit denen das RNN initialisert und ausgeführt werden kann. Der Aufbau ist auch hier stark an die Vorlage aus dem Pytorch-Grundlage angelehnt, da diese leicht verständlich und dem theoretischen Prinzip eines RNN am ehesten entsprach. Allerdings haben wir, wie bereits in unserem Exposé beschrieben, andere Aktivierungsfunktionen eingefügt, die beliebig ausgetauscht werden können, um die, je nach Aktivierungsfunktion, entstandenen Ergebnisse vergleichen zu können. Auf diesem Wege können die Auswirkungen der verschiedenen Aktivierungsfunktionen besser begutachtet werden. Auch haben wir uns an den letzten Vorlesungen orientiert und ein Dropout eingefügt, welcher besagt, wie hoch der Prozentsatz der inaktiven Neuronen pro Durchlauf sein soll. Auch hier bietet sich ein Verändern des Parameters an, um die Auswirkungen an den erstellten Passwörtern des Neuronalen Netzes zu beobachten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "class PasswordGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PasswordGenerator, self).__init__()\n",
    "        self.hidden = hidden_size\n",
    "        self.input2hidden = nn.Linear(input_size + hidden_size, hidden_size, bias=True)\n",
    "        self.input2output = nn.Linear(input_size + hidden_size, output_size, bias=True)\n",
    "        self.output2output = nn.Linear(hidden_size +  output_size, output_size, bias=True)        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined_input = torch.cat((input, hidden), dim=1)\n",
    "        hidden = self.input2hidden(combined_input)\n",
    "        output = self.input2output(combined_input)\n",
    "        combined_output = torch.cat((hidden, output), dim=1)\n",
    "        output = self. output2output(combined_output)        \n",
    "        output = self.dropout(output)\n",
    "        #output = self.softmax(output)\n",
    "        \n",
    "        #output = self.sigmoid(output)\n",
    "        output = self.relu(output)\n",
    "        #output = self.tanh(output)\n",
    "        \n",
    "        return hidden, output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktionen\n",
    "    nn.BCELoss() - Binary Cross Entropy\n",
    "    nn.BCEWithLogitsLoss() - This loss combines a Sigmoid layer and the BCELoss in one single class\n",
    "    nn.NLLLoss() - negative log likelihood loss\n",
    "    nn.CrossEntropyLoss() - combines nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
    "    \n",
    "## Training:\n",
    "\n",
    "Beim Training definieren wir zunächst unser Model, in dem wir den Inputgröße, die Anzahl der Hidden Layers und die Größe des Outputs angeben.\n",
    "Durch die Bindung der Loss-Function an einen Parameter lassen sich auch hier bequem andere Loss-Functions testen, um die Auswirkungen dieser auf den Trainingsprozess zu beobachten.\n",
    "/ Was macht unsqueeze mit -1?\n",
    "Aus Tutorials haben wir erfahren, dass die Methode zero_grad() unbedingt in der Trainingsmethode vorhanden sein muss, um den berechneten Fehler nicht zu addieren. Damit wird sichergestellt, dass der Fehler wieder zurückgesetzt wird und neue Werte annehmen kann.\n",
    "In einer Schleife wird dann der Forward-Pass und die Berechnung des Loss ausgeführt. Nach der Schleife kann der berechnete Fehler zurückgerechnet werden, um die Hyperparameter anzupassen.\n",
    "Ebenfalls haben wir eine kleine Funktion aus dem Pytorch-Tutorial übernommen, um Plot-Daten aus dem Model zu extrahieren.\n",
    "In der übergeordneten train() - Methode nehmen wir uns zufällige Passwörter aus der Liste, um mit diesen unser Netz zu trainieren. Diese ausgewählten Passwörter werden dann, wie eingangs erwähnt, zu Tensoren umgewandelt, damit die entsprechenden Methoden zum Training anwendbar sind. Diese Tensoren werden nun trainPasswords() übergeben, um anschließend den Gesamtfehler zu erhalten.\n",
    "\n",
    "//Abschnitt mit progress und c erklären"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PasswordGenerator(len_letters, len_letters, len_letters)\n",
    "loss_fn = nn.CrossEntropyLoss() #define Loss Function\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def trainPasswords(input, target):\n",
    "    target.unsqueeze_(-1)\n",
    "    hidden = model.initHidden()\n",
    "    model.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    loss = 0\n",
    "    for i in range(input.size()[0]):\n",
    "        hidden, output = model(input[i], hidden)\n",
    "        l = loss_fn(output, target[i]) # Compute the loss\n",
    "        loss += l\n",
    "    loss.requires_grad_(True) # The autograd package provides automatic differentiation for all operations on Tensors\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.add(-learning_rate, p.grad.data)\n",
    "        \n",
    "    return output, loss.item() / input.size(0)\n",
    "\n",
    "def train(trainrounds):\n",
    "    total_loss = 0\n",
    "    plots = []\n",
    "    plot_every = 100\n",
    "    progress = 0\n",
    "    c = 0\n",
    "    \n",
    "    for j in range(0, trainrounds):\n",
    "    #for j in range(len(passwords)):\n",
    "            #password = passwords[j]\n",
    "            password = random.choice(passwords)\n",
    "            #print('picked password:', password)\n",
    "            input = passwordToTensor(password)\n",
    "            target = targetToTensor(password)\n",
    "            output, loss = trainPasswords(input, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            progress = j / trainrounds * 100\n",
    "            if (c < round(progress) and round(progress) % 5 == 0) or j == 1:\n",
    "                c = round(progress)\n",
    "                print(round(progress), '% made. Loss: ', loss)\n",
    "            if j % plot_every == 0:\n",
    "                plots.append(total_loss / plot_every)\n",
    "                total_loss = 0\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Password:\n",
    "\n",
    "In diesem Bereich angekommen, haben wir bereits das Netz vollständig trainiere und können uns Passwörter generieren lassen.\n",
    "Da, wie Anfangs erklärt, die Hidden Layer der vorherigen Sequenz Einfluß nehmen auf die Hidden Layer der aktuellen Sequenz, bedeutet dies gleichzeitig, dass beim ersten Durchgang kein vorheriger Hidden Layer existiert. Daher geben wir den ersten Input vor, um dessen anschließende Hidden Layers an die nächste Sequenz zu übergeben.\n",
    "\n",
    "/ Rest aus der Schleife noch erklären"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Password:  rGYr;.t.t.t\n",
      "0 % made. Loss:  4.552676816529866\n",
      "5 % made. Loss:  nan\n"
     ]
    }
   ],
   "source": [
    "max_chars = 10 # max 10 chars for password\n",
    "\n",
    "def sample(start_letter='a'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = passwordToTensor(start_letter)\n",
    "        hidden = model.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_chars):\n",
    "            output, hidden = model(input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == len_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = letters[topi]\n",
    "                output_name += letter\n",
    "            input = passwordToTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "#before train\n",
    "random_start_char = random.choice(letters)\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "\n",
    "#train\n",
    "#train(range(len(passwords)))\n",
    "train(100000)\n",
    "\n",
    "#after train\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "print('Sampled Password: ', sample(random_start_char))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
