{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passwort Generator\n",
    "Das Projekt hat sich die Erstellung eines Passwort-Generators als Ziel gesetzt. Dabei sollen die zu erstellenden Passwörter möglichst denen eines Menschen ähneln. \n",
    "Zur Realisierung wurde die RNN-Architektur verwendet. Diese Recurrent Neural Networks ermöglichen es, Voraussagen mittels eines Kontext zu treffen, der durch frühere Inputs entstanden ist. Das Netzwerk verfügt sozusagen über ein Gedächtnis. In der Umsetzung geschieht dies durch die Kombination von Hidden-Layern aus der vorherigen Sequenz mit den Hidden-Layern aus der aktuellen Sequenz. Die vorherigen Hidden-Layer haben damit Einfluß auf den Output der nächsten Sequenz. Dieser Algorithmus wird in einer Schleife abgebildet, bis sämtliche Inputs verarbeitet wurden und der Kontext ersichtlich ist.\n",
    "Klarer wird dies mit den nachfolgenden Formeln, mit denen das Netzwerk trainiert wird:\n",
    "Quelle [2]\n",
    "\\begin{align}\n",
    "\\ h_t  = f(W^{hh}h_t-1 + W^{hx} + x_t \\\\\n",
    "\\ y_t  = softmax(W^Sh_t) \\\\\n",
    "\\ J^t(\\theta)  =\\sum_{i=1}^{[V]} (y_{ti}\\log(y_{ti}))\n",
    "\\end{align}\n",
    "\n",
    "Die erste Formel ist dafür da, sich an die Hidden-Layer des vorherigen Durchlaufs zu \"erinnern\". Dabei wird durch h-1 auf den vorherigen Hidden-Layer zugegriffen. Dies wird kombiniert mit dem aktuellen x, anschließend wird eine Akivierungsfunktion durchgeführt, am gebräuchlisten sind hierbei der Tangens hyperbolicus oder die Sigmoid-Funktion.\n",
    "Die zweite Formel kümmert sich um die Voraussage des nächsten Ergebnisses in Form von einer Wahrscheinlichkeitsverteilung. \n",
    "Zum Schluss wird in der dritten Formel mittels der Cross-Entropy-Loss-Funktion der Fehler zwischen dem Input und dem Output berechnet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from io import open #read Passwordlists\n",
    "import wget # to download passwordlist\n",
    "import os #check if file exists\n",
    "import random # pick random passwords from lists\n",
    "import unicodedata # convert passwords to unicode\n",
    "import string # pick chars\n",
    "import matplotlib.pyplot as plt # plot loss\n",
    "#Pytorch:\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auslesen der Passworddatei:\n",
    "\n",
    "Beim Einlesen der Daten haben wir uns an das Tutorial von Pytorch im Kontext mit RNNs gehalten. Um die Strings verarbeiten zu können, haben wir die Methode aus dem Tutorial übernommen, die die Zeichen in Ascii-Code umwandelt, um eine numerische Darstellung der Passwörter zu erhalten. Weiterhin erfolgt eine Umwandlung zu Tensoren durch One-Hot-Encoding sowohl für die Passwörter als auch für die Targets, damit die Backpropagation angewendet werden kann, um den Loss zu berechnen. [1]\n",
    "\n",
    "Die Länge der Passwortliste wird noch um eine weitere Stelle erweitert, um einen EOS-Marker (End of sentence) hinzuzufügen.\n",
    "Wie eingangs erwähnt, werden beim Training sowohl der vorherige als auch der aktuelle Hidden Layer berücksichtigt, eine Verarbeitung findet also in Tupeln statt: \n",
    "Beispiel \"A1B2\":\n",
    "Schritt 1:(A,1)\n",
    "Schritt 2:(1,B)\n",
    "Schritt 3:(B,2)\n",
    "Schritt 4:(2, EOS)\n",
    "Mit diesem Marker ist dem Netzwerk bekannt, wann das Ende des Passworts erreicht ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in letters\n",
    "    )\n",
    "# Auslesen der Passwortdatei\n",
    "def readPasswords(filename):\n",
    "    passwords = []\n",
    "    with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            if len(line) > 1:\n",
    "                passwords.append(line)\n",
    "    passw = [unicodeToAscii(password) for password in passwords]\n",
    "    return passw\n",
    "\n",
    "def convertPasswortToTensor(pw):\n",
    "    tensor = torch.zeros(len(pw), 1, len_letters)\n",
    "    for li in range(len(pw)):\n",
    "        letter = pw[li]\n",
    "        tensor[li][0][letters.find(letter)] = 1\n",
    "    if use_cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def convertTargetToTensor(password):\n",
    "    indizes = [letters.find(password[i]) for i in range(1,len(password))]\n",
    "    indizes.append(len_letters - 1) #EOS - Marker\n",
    "    indizes = torch.LongTensor(indizes)\n",
    "    if use_cuda:\n",
    "        indizes = indizes.cuda()\n",
    "    return indizes\n",
    "\n",
    "\n",
    "letters = string.ascii_letters + string.digits + string.punctuation\n",
    "len_letters = len(letters) + 1 # EOS - Marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Passwortlisten\n",
    "\n",
    "Um unser Projekt möglichst leicht reproduzierbar zu machen, haben wir eine Funktion eingebaut, die automatisch Passwortlisten runterlädt, falls diese noch nicht vorhanden sind. So ist gewährleistet, dass Interessenten, die selbst das Netzwerk trainieren möchten, nicht erst umständlich Passwortlisten runterladen müssen. Die Passwortlisten wurden vorher durch ein Skript von Passwörtern gereinigt, die Zeichen enthielten, die nicht UTF-8 -kompatibel waren, da diese Passwörter beim Einlesen Fehler erzeugt haben. Zusammen kommen wir auf ungefähr 46 Millionen Passwörter, die wir für das Training des Netzwerks verwenden könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful read rockyou.txt\n",
      "total passswords in list:  14344384\n",
      "successful read thelist.txt\n",
      "total passswords in list:  32310169\n"
     ]
    }
   ],
   "source": [
    "urls = ['https://www.scrapmaker.com/data/wordlists/dictionaries/rockyou.txt',\n",
    "        'https://www.scrapmaker.com/download/data/wordlists/passwords/thelist.txt']\n",
    "\n",
    "filelist = []\n",
    "passwords = []\n",
    "\n",
    "# exist files\n",
    "dirs = os.listdir()\n",
    "for file in dirs:\n",
    "    if file.endswith(\".txt\"):\n",
    "        filelist.append(file)\n",
    "\n",
    "for url in urls:\n",
    "    file = url.split(\"/\")[-1]\n",
    "    #download files if not exists\n",
    "    if file not in filelist:\n",
    "        wget.download(url)\n",
    "        print('\\nsuccessful downloaded ', url)\n",
    "    #read file and append to passwordlist\n",
    "    passwords += readPasswords(file)\n",
    "    print('successful read', file)\n",
    "    print('total passswords in list: ', len(passwords))\n",
    "\n",
    "#entfernen von leeren Zeilen\n",
    "passwords = [passw for passw in passwords if passw != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen\n",
    "Die hier aufgeführten Aktivierungsfunktionen werden mit dem init des Models erzeugt und können beliebig im Forward-Path verwendet werden.\n",
    "- nn.LogSoftmax() -  [6]\n",
    "\\begin{align}\n",
    "\\ LogSoftmax(x_i)  = log\\Bigl( \\frac{exp(x_i)}{\\sum_j exp(x_j)} \\Bigr) \\\\\n",
    "\\end{align}\n",
    "- nn.LeakyReLU() -  [7]\n",
    "\\begin{align}\n",
    "\\ LeakyRELU(x) = max(0,x) + negative\\_slope * min(0,x) \\\\\n",
    "\\end{align}\n",
    "\n",
    "- nn.LogSigmoid() - [8]\n",
    "\\begin{align}\n",
    "\\ LogSigmoid(x)  = log\\Bigl( \\frac{1)}{1+exp(-x} \\Bigr) \\\\\n",
    "\\end{align}\n",
    "- nn.Tanh() -  [9]\n",
    "\\begin{align}\n",
    "\\ tanh(x)  = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "## Generatorklasse:\n",
    "\n",
    "In der Generatorklasse werden die grundlegenden Funktionen und Variablen festgelegt, mit denen das RNN initialisert und ausgeführt werden kann. Der Aufbau ist auch hier stark an die Vorlage aus dem Pytorch-Grundlage angelehnt, da diese leicht verständlich und dem theoretischen Prinzip eines RNN am ehesten entsprach. Allerdings haben wir, wie bereits in unserem Exposé beschrieben, andere Aktivierungsfunktionen eingefügt, die beliebig ausgetauscht werden können, um die, je nach Aktivierungsfunktion, entstandenen Ergebnisse vergleichen zu können. Auf diesem Wege können die Auswirkungen der verschiedenen Aktivierungsfunktionen besser begutachtet werden. Auch haben wir uns an den Vorlesungen orientiert und ein Dropout eingefügt, welcher besagt, wie hoch der Prozentsatz der inaktiven Neuronen pro Durchlauf sein soll. Auch hier bietet sich ein Verändern des Parameters an, um die Auswirkungen an den erstellten Passwörtern des Neuronalen Netzes zu beobachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e27b513be9a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPasswordGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPasswordGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "class PasswordGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PasswordGenerator, self).__init__()\n",
    "        self.hidden = hidden_size\n",
    "        self.input2hidden = nn.Linear(input_size + hidden_size, hidden_size, bias=True)\n",
    "        self.input2output = nn.Linear(input_size + hidden_size, output_size, bias=True)\n",
    "        self.output2output = nn.Linear(hidden_size +  output_size, output_size, bias=True)        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined_input = torch.cat((input, hidden), dim=1)\n",
    "        hidden = self.input2hidden(combined_input)\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        output = self.input2output(combined_input)\n",
    "        output = self.relu(output)\n",
    "        combined_output = torch.cat((hidden, output), dim=1)\n",
    "        output = self. output2output(combined_output)        \n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        #output = self.sigmoid(output)\n",
    "        \n",
    "        return hidden, output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA einrichten\n",
    "Um das Training nicht auf der CPU sondern im schnelleren GPU durchzuführen, haben wir CUDA (Compute Unified Device Architecture) aktiviert. CUDA ist eine von Nvidia entwickelte Programmier-Technik, mit der Programmteile durch den Grafikprozessor (GPU) abgearbeitet werden können. <br>\n",
    "Im folgenden wird geprüft ob CUDA aktiviert werden kann und eine entsprechende Vairable gesetzt. Wenn CUDA Verfügbar ist, werden im weiteren Code die Tensoren und das Model in den GPU geschrieben. <br>\n",
    "Eine Verbesserung, die wir jedoch Zeitlich nicht mehr geschafft haben, ist die Einbindung von Multiprocessing um mehrere Prozesse im GPU zu erzeugen und ein schnelleres Training zu ermöglichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA activated\n",
      "GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.init() # Does nothing if the CUDA state is already initialized.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('CUDA activated')\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    device = torch.device(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktionen\n",
    "- nn.BCELoss() - Binary Cross Entropy [10]\n",
    "Wenn \"reduction = none\":\n",
    "\n",
    "\\begin{align}\n",
    "\\ l_n(x,y) = -w_n [y_n \\times logx_n + (1-y_n) \\times log(1-x_n)] \\\\\n",
    "\\end{align}\n",
    "N = Batch-size\n",
    "\n",
    "Wenn \"reduction != none\":\n",
    "\\begin{align}\n",
    "l(x,y) =\n",
    "\\begin{vmatrix}\n",
    "\\    (mean(L) & if reduction='mean' \\\\\n",
    "sum(L) & if reduction='sum'   \\\\\n",
    "\\end{vmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- nn.BCEWithLogitsLoss() - This loss combines a Sigmoid layer and the BCELoss in one single class [11]\n",
    "\n",
    "Wenn \"reduction = none\":\n",
    "\\begin{align}\n",
    "\\ l_n(x,y) = -w_n [y_n*log\\sigma(x_n) + (1-y_n)*log(1-\\sigma(x_n))] \\\\\n",
    "\\end{align}\n",
    "N = Batch-size\n",
    "\n",
    "Wenn \"reduction != none\":\n",
    "\\begin{align}\n",
    "l(x,y) =\n",
    "\\begin{vmatrix}\n",
    "\\    (mean(L) & if reduction='mean' \\\\\n",
    "sum(L) & if reduction='sum'   \\\\\n",
    "\\end{vmatrix}\n",
    "\\end{align}\n",
    "\n",
    "- nn.NLLLoss() - negative log likelihood loss [12]\n",
    "\n",
    "\\begin{align}\n",
    "\\ l_n(x,y) = -w_{y_n} x_{x,y_n} \\\\\n",
    "\\end{align}\n",
    "N = Batch-size\n",
    "\n",
    "Wenn \"reduction != none\":\n",
    "\\begin{align}\n",
    "l(x,y) =\n",
    "\\begin{vmatrix}\n",
    "\\    \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n & if reduction='mean' \\\\\n",
    "\\sum_{n=1}^N l_n & if reduction='sum'   \\\\\n",
    "\\end{vmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- nn.CrossEntropyLoss() - combines nn.LogSoftmax() and nn.NLLLoss() in one single class [13] \n",
    "\n",
    "Kein weight spezifiziert: \n",
    "\\begin{align}\n",
    "\\ loss(x, class) = -log \\Bigl( \\frac{exp(x[class])}{\\sum_j exp(x[j])} \\Bigr) = -x[class] + log \\Bigl(sum_j exp(x[j] \\Bigr) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Weight spezifiziert: \n",
    "\n",
    "\\begin{align}\n",
    "\\ loss(x, class) = weight[class]  \\Bigl( -x[class] + log \\Bigl(\\sum_j exp(x[j]) \\Bigr) \\Bigr) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "    \n",
    "## Training:\n",
    "\n",
    "Beim Training definieren wir unser Model, indem wir die Inputgröße, die Anzahl der Hidden Layers und die Größe des Outputs angeben.\n",
    "Durch die Bindung der Loss-Function an einen Parameter lassen sich auch hier bequem andere Loss-Functions testen, um die Auswirkungen dieser auf den Trainingsprozess zu beobachten.<br>\n",
    "Aus Tutorials haben wir erfahren, dass die Methode zero_grad() unbedingt in der Trainingsmethode vorhanden sein muss, um den berechneten Fehler nicht zu addieren. Damit wird sichergestellt, dass der Fehler wieder zurückgesetzt wird und neue Werte annehmen kann.\n",
    "In einer Schleife wird dann der Forward-Pass und die Berechnung des Loss ausgeführt. Nach der Schleife kann der berechnete Fehler zurückgerechnet werden, um die Hyperparameter anzupassen.\n",
    "Ebenfalls haben wir eine kleine Funktion aus dem Pytorch-Tutorial übernommen, um Plot-Daten aus dem Model zu extrahieren. Die berechneten Loss-Werte werden hier als Funktion geplottet.<br>\n",
    "In der übergeordneten train() - Methode nehmen wir uns zufällige Passwörter aus der Liste, um mit diesen unser Netz zu trainieren. Diese ausgewählten Passwörter werden dann, wie eingangs erwähnt, zu Tensoren umgewandelt, damit die entsprechenden Methoden zum Training anwendbar sind. Diese Tensoren werden nun trainPasswords() übergeben, um anschließend den Gesamtfehler zu erhalten. <br>\n",
    "Über den Parameter trainrounds kann bestimmt werden wie viele zufällige Passwörter aus der Liste gezogen werden, um damit zu trainieren. Man könnte hier natürlich auch einfach alle Passwörter aus der Liste der Reihe nach durchlaufen lassen, allerdings brauchen 100000 Passworte in unserem Test bereits knapp 4 Stunden. Hier müsste noch die Performance verbessert werden. Daher mussten wir beim Training einen Kompromiss zwischen vernünftigen Ergebnissen und ausreichender Trainingsdauer finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PasswordGenerator(len_letters, len_letters, len_letters)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss() #define Loss Function\n",
    "learning_rate = 0.001\n",
    "\n",
    "def trainPasswords(input, target):\n",
    "    target.unsqueeze_(-1) # entfernen der letzten Dimension\n",
    "    hidden = model.initHidden()\n",
    "    model.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    if use_cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    loss = 0\n",
    "    for i in range(input.size()[0]):\n",
    "        hidden, output = model(input[i], hidden)\n",
    "        l = loss_fn(output, target[i]) # Compute the loss\n",
    "        loss += l\n",
    "    loss.requires_grad_(True) # The autograd package provides automatic differentiation for all operations on Tensors\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.add(-learning_rate, p.grad.data)\n",
    "        \n",
    "    return output, loss.item() / input.size(0)\n",
    "\n",
    "def train(trainrounds):\n",
    "    total_loss = 0\n",
    "    plots = []\n",
    "    plot_every = 100\n",
    "    progress = 0\n",
    "    c = 0\n",
    "    \n",
    "    for j in range(0, trainrounds):\n",
    "            password = random.choice(passwords)\n",
    "            #print('picked password:', password)\n",
    "            input = convertPasswortToTensor(password)\n",
    "            target = convertTargetToTensor(password)\n",
    "            output, loss = trainPasswords(input, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            progress = j / trainrounds * 100\n",
    "            if (c < round(progress) and round(progress) % 5 == 0) or j == 1:\n",
    "                #if use_cuda:\n",
    "                    #print('the current GPU memory occupied by tensors (GB): ', torch.cuda.memory_allocated() / 1024 / 1024)\n",
    "                c = round(progress)\n",
    "                print(round(progress), '% made. Loss: ', loss)\n",
    "            if j % plot_every == 0:\n",
    "                plots.append(total_loss / plot_every)\n",
    "                total_loss = 0\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Password:\n",
    "\n",
    "In diesem Bereich angekommen, haben wir bereits das Netz vollständig trainiert und können uns Passwörter generieren lassen.\n",
    "Da, wie Anfangs erklärt, die Hidden Layer der vorherigen Sequenz Einfluß nehmen auf die Hidden Layer der aktuellen Sequenz, bedeutet dies gleichzeitig, dass beim ersten Durchgang kein vorheriger Hidden Layer existiert. Daher geben wir den ersten Input vor, um dessen anschließende Hidden Layers an die nächste Sequenz zu übergeben.\n",
    "<br>\n",
    "In der Schleife zur Gewinnung eines Samples wird zunächst das Model initialisiert und durch die Methode topk() wird der Buchstabe mit der höchsten Wahrscheinlichkeit unserem Sample hinzugefügt. <br>\n",
    "Um einen Unterschied durch das Training zu zeigen, wird die sample()-Methode einmal vor und ein paar mal nach dem Training aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Password:  hcWWjjWjjjWWW\n",
      "0 % made. Loss:  4.589341799418132\n",
      "5 % made. Loss:  3.8004014756944446\n",
      "10 % made. Loss:  3.4839646021525064\n",
      "15 % made. Loss:  3.3218360900878907\n",
      "20 % made. Loss:  3.3301265716552733\n",
      "25 % made. Loss:  3.0973897661481584\n",
      "30 % made. Loss:  3.8945229848225913\n",
      "35 % made. Loss:  2.9916783650716146\n",
      "40 % made. Loss:  3.472919209798177\n",
      "45 % made. Loss:  3.537563959757487\n",
      "50 % made. Loss:  2.84042845831977\n",
      "55 % made. Loss:  3.2649238109588623\n",
      "60 % made. Loss:  2.526250203450521\n",
      "65 % made. Loss:  3.5015176137288413\n",
      "70 % made. Loss:  4.209789594014485\n",
      "75 % made. Loss:  3.4841396808624268\n",
      "80 % made. Loss:  2.5727745056152345\n",
      "85 % made. Loss:  3.637464761734009\n",
      "90 % made. Loss:  4.212200437273298\n",
      "95 % made. Loss:  3.40599365234375\n",
      "100 % made. Loss:  4.248691082000732\n",
      "Sampled Password 01:  hSaaaaaaaaaaa\n",
      "Sampled Password 02:  hSaaaaaaaaaaa\n",
      "Sampled Password 03:  hSaaaaaaaaaaa\n",
      "Sampled Password 04:  hSaaaaaaaaaaa\n",
      "Sampled Password 05:  hSaaaaaaaaaaa\n",
      "Sampled Password 06:  hSaaaaaaaaaaa\n",
      "Sampled Password 07:  hSaaaaaaaaaaa\n",
      "Sampled Password 08:  hSaaaaaaaaaaa\n",
      "Sampled Password 09:  hSaaaaaaaaaaa\n",
      "Sampled Password 10:  hSaaaaaaaaaaa\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_chars = 12 # max 10 chars for password\n",
    "\n",
    "def sample(start_letter='a'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = convertPasswortToTensor(start_letter)\n",
    "        hidden = model.initHidden()\n",
    "        if use_cuda:\n",
    "            hidden = hidden.cuda()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_chars):\n",
    "            output, hidden = model(input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == len_letters - 1: \n",
    "                break\n",
    "            else:\n",
    "                letter = letters[topi]\n",
    "                output_name += letter\n",
    "            input = convertPasswortToTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "#before train\n",
    "random_start_char = random.choice(letters)\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "\n",
    "train(100000)\n",
    "\n",
    "#after train\n",
    "print('Sampled Password 01: ', sample(random_start_char))\n",
    "print('Sampled Password 02: ', sample(random_start_char))\n",
    "print('Sampled Password 03: ', sample(random_start_char))\n",
    "print('Sampled Password 04: ', sample(random_start_char))\n",
    "print('Sampled Password 05: ', sample(random_start_char))\n",
    "print('Sampled Password 06: ', sample(random_start_char))\n",
    "print('Sampled Password 07: ', sample(random_start_char))\n",
    "print('Sampled Password 08: ', sample(random_start_char))\n",
    "print('Sampled Password 09: ', sample(random_start_char))\n",
    "print('Sampled Password 10: ', sample(random_start_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "Abschließend lässt sich sagen, dass es relativ einfach ist mit einiger Einarbeitung einen Passwort Generator mit Pytorch zu schreiben. Verbesserungswürdig ist an dieser Stelle allerdings die Performance, welche mit Multiprocessing erhöt werden könnte. <br>\n",
    "Unsere Tests ergaben, dass wenn Lernrate und Dropout zu hoch oder niedrig sind, die Passwörter nur auf den ersten paar Zeichen einem guten Passwort entsprechen. Außerdem scheinen eine Droprate von 0.2 und eine Lernrate von 0.001 die besten Ergebnisse zu liefern. Des Weiteren können wir sagen, dass unter 10000 Trainingsdurchgänge keine sinnvollen Ergebnisse produziert werden können. <br>\n",
    "Im Tutorial wurde im Netz nur eine Softmax Aktivierungsfunktion verwendet. Um das Netz ein wenig komplexer zu machen, haben wir auf die Hiddens eine LogSigmoid Funktion und auf den kombinierten Input eine LeakyReLU Funktion. Dies lieferte wesentlich bessere Ergebnisse als eine einzige Aktivierungsfunktion. <br>\n",
    "Auch wäre im Zuge einer Weiterentwicklung die Verwendung einer LSTM - Architektur(Long short-term memory) vorteilhaft, da diese eine der Schwächen von RNNs, die kurze Erinnerungsdauer, behebt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen\n",
    "\n",
    "[1] https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html <br>\n",
    "[2] https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7 <br>\n",
    "[3] https://www.scrapmaker.com/data/wordlists/dictionaries/rockyou.txt <br>\n",
    "[4] https://www.scrapmaker.com/download/data/wordlists/passwords/thelist.txt <br>\n",
    "[5] https://arxiv.org/pdf/1308.0850.pdf <br>\n",
    "Aktivierungsfunktionen:\n",
    "[6]https://pytorch.org/docs/stable/nn.html#logsoftmax <br>\n",
    "[7]https://pytorch.org/docs/stable/nn.html#leakyrelu <br>\n",
    "[8]https://pytorch.org/docs/stable/nn.html#logsigmoid <br>\n",
    "[9]https://pytorch.org/docs/stable/nn.html#tanh <br>\n",
    "\n",
    "Lossfunktionen:\n",
    "[10]https://pytorch.org/docs/stable/nn.html#bceloss <br>\n",
    "[11]https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss <br>\n",
    "[12]https://pytorch.org/docs/stable/nn.html#nllloss <br>\n",
    "[13]https://pytorch.org/docs/stable/nn.html#crossentropyloss <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
