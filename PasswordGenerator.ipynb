{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passwort Generator\n",
    "Das Projekt hat sich die Erstellung eines Passwort-Generators als Ziel gesetzt. Dabei sollen die zu erstellten Passwörter möglichst denen eines Menschen ähneln. \n",
    "Zur Realisierung wurde die RNN-Architektur verwendet. Diese Recurrent Neural Networks ermöglichen es, Voraussagen mittels eines Kontext zu treffen, der durch frühere Inputs entstanden ist. Das Netzwerk verfügt sozusagen über ein Gedächtnis. In der Umsetzung geschieht dies durch die Kombination von Hidden-Layern aus der vorherigen Sequenz mit den Hidden-Layern aus der aktuellen Sequenz. Die vorherigen Hidden-Layer haben damit Einfluß auf den Output der nächsten Sequenz. Dieser Algorithmus wird in einer Schleife abgebildet, bis sämtliche Inputs verarbeitet wurden und der Kontext ersichtlich ist.\n",
    "Klarer wird dies mit den nachfolgenden Formeln, mit denen das Netzwerk trainiert wird:\n",
    "Quelle [2]\n",
    "\\begin{align}\n",
    "\\ h_t  = f(W^{hh}h_t-1 + W^{hx} + x_t \\\\\n",
    "\\ y_t  = softmax(W^Sh_t) \\\\\n",
    "\\ J^t(\\theta)  =\\sum_{i=1}^{[V]} (y_{ti}\\log(y_{ti}))\n",
    "\\end{align}\n",
    "\n",
    "Die erste Formel ist dafür da, sich an die Hidden-Layer des vorherigen Durchlaufs zu \"erinnern\". Dabei wird durch h-1 auf den vorherigen Hidden-Layer zugegriffen. Dies wird kombiniert mit dem aktuellen x, anschließend eine Akivierungsfunktion durchgeführt, am gebräuchlisten sind hierbei der Tangens hyperbolicus oder die Sigmoid-Funktion.\n",
    "Die zweite Formel kümmert sich um die Voraussage des nächsten Ergebnisses in Form von einer Wahrscheinlichkeitsverteilung. \n",
    "Zum Schluss wird in der dritten Formel mittels der Cross-Entropy-Loss-Funktion der Fehler zwischen dem Input und dem Output berechnet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import wget # to download passwordlist\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auslesen der Passworddatei:\n",
    "\n",
    "Beim Einlesen der Daten haben wir uns an das Tutorial von Pytorch im Kontext mit RNNs gehalten. Um die Strings verarbeiten zu können, haben wir die Methode aus dem Tutorial übernommen, die die Zeichen in Ascii-Code umwandelt, um eine numerische Darstellung der Passwörter zu erhalten. Weiterhin erfolgt eine Umwandlung zu Tensoren durch One-Hot-Encoding sowohl für die Passwörter als auch für die Targets, damit die Backpropagation angewendet werden kann, um den Loss zu berechnen.\n",
    "\n",
    "Der Länge der Passwortliste wird noch um eine weitere Stelle erweitert, um einen EOS-Marker (End of sentence) hinzuzufügen.\n",
    "Wie eingangs erwähnt, werden beim Training sowohl der vorherige als auch der aktuelle Hidden Layer berücksichtigt, eine Verarbeitung findet also in Tupeln statt: \n",
    "Beispiel \"A1B2\":\n",
    "Schritt 1:(A,1)\n",
    "Schritt 2:(1,B)\n",
    "Schritt 3:(B,2)\n",
    "Schritt 4:(2, EOS)\n",
    "Mit diesem Marker ist dem Netzwerk bekannt, wann das Ende des Passworts erreicht ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in letters\n",
    "    )\n",
    "# Auslesen der Passwortdatei\n",
    "def readPasswords(filename):\n",
    "    passwords = []\n",
    "    with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            if len(line) > 1:\n",
    "                passwords.append(line)\n",
    "    passw = [unicodeToAscii(password) for password in passwords]\n",
    "    return passw\n",
    "\n",
    "def convertPasswortToTensor(pw):\n",
    "    tensor = torch.zeros(len(pw), 1, len_letters)\n",
    "    for li in range(len(pw)):\n",
    "        letter = pw[li]\n",
    "        tensor[li][0][letters.find(letter)] = 1\n",
    "    if use_cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def convertTargetToTensor(password):\n",
    "    indizes = [letters.find(password[i]) for i in range(1,len(password))]\n",
    "    indizes.append(len_letters - 1) #EOS - Marker\n",
    "    indizes = torch.LongTensor(indizes)\n",
    "    if use_cuda:\n",
    "        indizes = indizes.cuda()\n",
    "    return indizes\n",
    "\n",
    "\n",
    "letters = string.ascii_letters + string.digits + string.punctuation\n",
    "len_letters = len(letters) + 1 # EOS - Marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Passwortlisten\n",
    "\n",
    "Um unser Projekt möglichst leicht reproduzierbar zu machen, haben wir eine Funktion eingebaut, die automatisch Passwortlisten runterlädt, falls diese noch nicht vorhanden sind. So ist gewährleistet, dass Interessenten, die selbst das Netzwerk trainieren möchten, nicht erst umständlich Passwortlisten runterladen müssen. Die Passwortlisten wurden vorher durch ein Skript von Passwörtern gereinigt, die Zeichen enthielten, die nicht UTF-8 -kompatibel waren, da diese Passwörter beim Einlesen Fehler erzeugt haben. Zusammen kommen wir auf ungefähr 83 Millionen Passwörter, die wir für das Training des Netzwerks verwenden könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful read rockyou.txt\n",
      "total passswords in list:  14344384\n",
      "successful read thelist.txt\n",
      "total passswords in list:  32310169\n"
     ]
    }
   ],
   "source": [
    "urls = ['https://www.scrapmaker.com/data/wordlists/dictionaries/rockyou.txt',\n",
    "        'https://www.scrapmaker.com/download/data/wordlists/passwords/thelist.txt']\n",
    "\n",
    "filelist = []\n",
    "passwords = []\n",
    "\n",
    "# exist files\n",
    "dirs = os.listdir()\n",
    "for file in dirs:\n",
    "    if file.endswith(\".txt\"):\n",
    "        filelist.append(file)\n",
    "\n",
    "for url in urls:\n",
    "    file = url.split(\"/\")[-1]\n",
    "    #download files if not exists\n",
    "    if file not in filelist:\n",
    "        wget.download(url)\n",
    "        print('\\nsuccessful downloaded ', url)\n",
    "    #read file and append to passwordlist\n",
    "    passwords += readPasswords(file)\n",
    "    print('successful read', file)\n",
    "    print('total passswords in list: ', len(passwords))\n",
    "\n",
    "#entfernen von leeren Zeilen\n",
    "passwords = [passw for passw in passwords if passw != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen\n",
    "Die hier aufgeführten Aktivierungsfunktionen werden mit dem init des Models erzeugt und können beliebig im Forward-Path verwendet werden.\n",
    "    nn.LogSoftmax() - \n",
    "    nn.LeakyReLU() - \n",
    "    nn.LogSigmoid() -\n",
    "    nn.Tanh() - \n",
    "## Generatorklasse:\n",
    "\n",
    "In der Generatorklasse werden die grundlegenden Funktionen und Variablen festgelegt, mit denen das RNN initialisert und ausgeführt werden kann. Der Aufbau ist auch hier stark an die Vorlage aus dem Pytorch-Grundlage angelehnt, da diese leicht verständlich und dem theoretischen Prinzip eines RNN am ehesten entsprach. Allerdings haben wir, wie bereits in unserem Exposé beschrieben, andere Aktivierungsfunktionen eingefügt, die beliebig ausgetauscht werden können, um die, je nach Aktivierungsfunktion, entstandenen Ergebnisse vergleichen zu können. Auf diesem Wege können die Auswirkungen der verschiedenen Aktivierungsfunktionen besser begutachtet werden. Auch haben wir uns an den Vorlesungen orientiert und ein Dropout eingefügt, welcher besagt, wie hoch der Prozentsatz der inaktiven Neuronen pro Durchlauf sein soll. Auch hier bietet sich ein Verändern des Parameters an, um die Auswirkungen an den erstellten Passwörtern des Neuronalen Netzes zu beobachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "class PasswordGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PasswordGenerator, self).__init__()\n",
    "        self.hidden = hidden_size\n",
    "        self.input2hidden = nn.Linear(input_size + hidden_size, hidden_size, bias=True)\n",
    "        self.input2output = nn.Linear(input_size + hidden_size, output_size, bias=True)\n",
    "        self.output2output = nn.Linear(hidden_size +  output_size, output_size, bias=True)        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined_input = torch.cat((input, hidden), dim=1)\n",
    "        hidden = self.input2hidden(combined_input)\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        output = self.input2output(combined_input)\n",
    "        output = self.relu(output)\n",
    "        combined_output = torch.cat((hidden, output), dim=1)\n",
    "        output = self. output2output(combined_output)        \n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        #output = self.sigmoid(output)\n",
    "        \n",
    "        return hidden, output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA einrichten\n",
    "Um das Training nicht auf der CPU sondern im schnelleren GPU durchzuführen, haben wir CUDA (Compute Unified Device Architecture) aktiviert. CUDA ist eine von Nvidia entwickelte Programmier-Technik, mit der Programmteile durch den Grafikprozessor (GPU) abgearbeitet werden können. <br>\n",
    "Im folgenden wird geprüft ob CUDA aktiviert werden kann und eine entsprechende Vairable gesetzt. Wenn CUDA Verfügbar ist, werden im weiteren Code die Tensoren und das Model in den GPU geschrieben. <br>\n",
    "Eine Verbesserung, die wir jedoch Zeitlich nicht mehr geschafft haben, ist die Einbindung von Multiprocessing um mehrere Prozesse im GPU zu erzeugen und ein schnelleres Training zu ermöglichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA activated\n",
      "GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.init() # Does nothing if the CUDA state is already initialized.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('CUDA activated')\n",
    "    #FloatTensor = torch.cuda.FloatTensor\n",
    "    #LongTensor = torch.cuda.LongTensor\n",
    "    #torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    device = torch.device(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktionen\n",
    "    nn.BCELoss() - Binary Cross Entropy\n",
    "    nn.BCEWithLogitsLoss() - This loss combines a Sigmoid layer and the BCELoss in one single class\n",
    "    nn.NLLLoss() - negative log likelihood loss\n",
    "    nn.CrossEntropyLoss() - combines nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
    "    \n",
    "## Training:\n",
    "\n",
    "Beim Training definieren wir unser Model, in dem wir die Inputgröße, die Anzahl der Hidden Layers und die Größe des Outputs angeben.\n",
    "Durch die Bindung der Loss-Function an einen Parameter lassen sich auch hier bequem andere Loss-Functions testen, um die Auswirkungen dieser auf den Trainingsprozess zu beobachten.<br>\n",
    "Aus Tutorials haben wir erfahren, dass die Methode zero_grad() unbedingt in der Trainingsmethode vorhanden sein muss, um den berechneten Fehler nicht zu addieren. Damit wird sichergestellt, dass der Fehler wieder zurückgesetzt wird und neue Werte annehmen kann.\n",
    "In einer Schleife wird dann der Forward-Pass und die Berechnung des Loss ausgeführt. Nach der Schleife kann der berechnete Fehler zurückgerechnet werden, um die Hyperparameter anzupassen.\n",
    "Ebenfalls haben wir eine kleine Funktion aus dem Pytorch-Tutorial übernommen, um Plot-Daten aus dem Model zu extrahieren. Die berechneten Loss-Werte werden hier als Funktion geplottet.<br>\n",
    "In der übergeordneten train() - Methode nehmen wir uns zufällige Passwörter aus der Liste, um mit diesen unser Netz zu trainieren. Diese ausgewählten Passwörter werden dann, wie eingangs erwähnt, zu Tensoren umgewandelt, damit die entsprechenden Methoden zum Training anwendbar sind. Diese Tensoren werden nun trainPasswords() übergeben, um anschließend den Gesamtfehler zu erhalten. <br>\n",
    "Über den Parameter trainrounds kann bestimmt werden wie viele zufällige Passwörter aus der Liste gezogen werden um damit zu trainieren. Man könnte hier natürlich auch einfach alle Passwörter aus der Liste der Reihe nach durchlaufen lassen, allerdings brauchen 100000 Passworte in unserem Test bereits knapp 4 Stunden. Hier müsste noch die Performance verbessert werden. Daher mussten wir beim Training einen Kompromiss zwischen vernünftigen Ergebnissen und ausreichender Trainingsdauer finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PasswordGenerator(len_letters, len_letters, len_letters)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss() #define Loss Function\n",
    "learning_rate = 0.001\n",
    "\n",
    "def trainPasswords(input, target):\n",
    "    target.unsqueeze_(-1) # entfernen der letzten Dimension\n",
    "    hidden = model.initHidden()\n",
    "    model.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    if use_cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    loss = 0\n",
    "    for i in range(input.size()[0]):\n",
    "        hidden, output = model(input[i], hidden)\n",
    "        l = loss_fn(output, target[i]) # Compute the loss\n",
    "        loss += l\n",
    "    loss.requires_grad_(True) # The autograd package provides automatic differentiation for all operations on Tensors\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.add(-learning_rate, p.grad.data)\n",
    "        \n",
    "    return output, loss.item() / input.size(0)\n",
    "\n",
    "def train(trainrounds):\n",
    "    total_loss = 0\n",
    "    plots = []\n",
    "    plot_every = 100\n",
    "    progress = 0\n",
    "    c = 0\n",
    "    \n",
    "    for j in range(0, trainrounds):\n",
    "            password = random.choice(passwords)\n",
    "            #print('picked password:', password)\n",
    "            input = convertPasswortToTensor(password)\n",
    "            target = convertTargetToTensor(password)\n",
    "            output, loss = trainPasswords(input, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            progress = j / trainrounds * 100\n",
    "            if (c < round(progress) and round(progress) % 5 == 0) or j == 1:\n",
    "                #if use_cuda:\n",
    "                    #print('the current GPU memory occupied by tensors (GB): ', torch.cuda.memory_allocated() / 1024 / 1024)\n",
    "                c = round(progress)\n",
    "                print(round(progress), '% made. Loss: ', loss)\n",
    "            if j % plot_every == 0:\n",
    "                plots.append(total_loss / plot_every)\n",
    "                total_loss = 0\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Password:\n",
    "\n",
    "In diesem Bereich angekommen, haben wir bereits das Netz vollständig trainiert und können uns Passwörter generieren lassen.\n",
    "Da, wie Anfangs erklärt, die Hidden Layer der vorherigen Sequenz Einfluß nehmen auf die Hidden Layer der aktuellen Sequenz, bedeutet dies gleichzeitig, dass beim ersten Durchgang kein vorheriger Hidden Layer existiert. Daher geben wir den ersten Input vor, um dessen anschließende Hidden Layers an die nächste Sequenz zu übergeben.\n",
    "<br>\n",
    "In der Schleife zur Gewinnung eines Samples wird zunächst das Model initialisiert und durch die Methode topk() wird der Buchstabe mit der höchsten Wahrscheinlichkeit unserem Sample hinzugefügt. <br>\n",
    "Um einen Unterschied durch das training zu zeigen, wird die sample Methode einmal vor und ein paar mal nach dem Training aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Password:  *\\ppppppppp\n",
      "0 % made. Loss:  4.663131020285866\n",
      "5 % made. Loss:  3.8746582667032876\n",
      "10 % made. Loss:  3.3712545541616588\n",
      "15 % made. Loss:  3.2487869262695312\n",
      "20 % made. Loss:  3.695100021362305\n",
      "25 % made. Loss:  3.4853649139404297\n",
      "30 % made. Loss:  3.0503942701551647\n",
      "35 % made. Loss:  3.3517794609069824\n",
      "40 % made. Loss:  3.2415809631347656\n",
      "45 % made. Loss:  3.879272143046061\n",
      "50 % made. Loss:  3.3667319615681968\n",
      "55 % made. Loss:  3.746443748474121\n",
      "60 % made. Loss:  3.879858652750651\n",
      "65 % made. Loss:  3.3346150716145835\n",
      "70 % made. Loss:  3.680746623447963\n",
      "75 % made. Loss:  3.652318318684896\n",
      "80 % made. Loss:  3.002714474995931\n",
      "85 % made. Loss:  3.0666420459747314\n",
      "90 % made. Loss:  2.9449148178100586\n",
      "95 % made. Loss:  3.9586308797200522\n",
      "100 % made. Loss:  3.608944574991862\n",
      "Sampled Password 01:  *\\ppbbbbbbb\n",
      "Sampled Password 02:  *\\ppcbbbbbb\n",
      "Sampled Password 03:  *\\ppbbbbbbb\n",
      "Sampled Password 04:  *\\ppcbbbbbb\n",
      "Sampled Password 05:  *\\ppcbbbbbb\n",
      "Sampled Password 06:  *\\ppcbbbbbb\n",
      "Sampled Password 07:  *\\ppcbbbbbb\n",
      "Sampled Password 08:  *\\ppcbbbbbb\n",
      "Sampled Password 09:  *\\ppbbbbbbb\n",
      "Sampled Password 10:  *\\ppcbbbbbb\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_chars = 10 # max 10 chars for password\n",
    "\n",
    "def sample(start_letter='a'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = convertPasswortToTensor(start_letter)\n",
    "        hidden = model.initHidden()\n",
    "        if use_cuda:\n",
    "            hidden = hidden.cuda()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_chars):\n",
    "            output, hidden = model(input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == len_letters - 1: \n",
    "                break\n",
    "            else:\n",
    "                letter = letters[topi]\n",
    "                output_name += letter\n",
    "            input = convertPasswortToTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "#before train\n",
    "random_start_char = random.choice(letters)\n",
    "print('Sampled Password: ', sample(random_start_char))\n",
    "\n",
    "train(100000)\n",
    "\n",
    "#after train\n",
    "print('Sampled Password 01: ', sample(random_start_char))\n",
    "print('Sampled Password 02: ', sample(random_start_char))\n",
    "print('Sampled Password 03: ', sample(random_start_char))\n",
    "print('Sampled Password 04: ', sample(random_start_char))\n",
    "print('Sampled Password 05: ', sample(random_start_char))\n",
    "print('Sampled Password 06: ', sample(random_start_char))\n",
    "print('Sampled Password 07: ', sample(random_start_char))\n",
    "print('Sampled Password 08: ', sample(random_start_char))\n",
    "print('Sampled Password 09: ', sample(random_start_char))\n",
    "print('Sampled Password 10: ', sample(random_start_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen\n",
    "\n",
    "[1] https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html <br>\n",
    "[2] https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7 <br>\n",
    "[3] https://www.scrapmaker.com/data/wordlists/dictionaries/rockyou.txt <br>\n",
    "[4] https://www.scrapmaker.com/download/data/wordlists/passwords/thelist.txt <br>\n",
    "[5] https://arxiv.org/pdf/1308.0850.pdf <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
